{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,) (1, 10) (10, 1)\n",
      "False False\n",
      "(1, 10) (10, 10) (10, 10)\n",
      "(1, 1) (10, 10)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5,6,7,8,9,10]) #rank 1 array which is neither a row vector (1,10) nor a column vector (10, 1)\n",
    "b = np.array([1,2,3,4,5,6,7,8,9,10]).reshape((1,10))\n",
    "c = np.array([1,2,3,4,5,6,7,8,9,10]).reshape((10, 1))\n",
    "print(a.shape, b.shape, c.shape)\n",
    "print(np.array_equal(a, b), np.array_equal(a, c))\n",
    "print((a*b).shape, (a*c).shape, (b*c).shape)\n",
    "print(np.dot(b,c).shape, np.dot(c,b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249966.59497044512\n",
      "Vectorized version:1.7580986022949219ms\n",
      "249966.59497044657\n",
      "For loop:556.1151504516602ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a,b)\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print(\"Vectorized version:\" + str(1000*(toc-tic)) +\"ms\" )\n",
    "\n",
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print(\"For loop:\" + str(1000*(toc-tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(10, 0)\n",
      "(0, 10)\n"
     ]
    }
   ],
   "source": [
    "print(np.zeros(10).shape)\n",
    "print(np.zeros((10, 0)).shape)\n",
    "print(np.zeros((0,10)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized version of Logistic Regression\n",
    "\n",
    "There are three for loops:\n",
    "* The outmost for loop is for the **i iterations**\n",
    "    * second for loop is for the **m training examples**\n",
    "        * third for loop is over the **nx weights**\n",
    "\n",
    "We can vectorize second and third for loops this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.41986636]\n",
      "[1.10393502]\n",
      "[0.88655215]\n",
      "[0.76997461]\n",
      "[0.72108426]\n",
      "[0.7036203]\n",
      "[0.69777363]\n",
      "[0.69580776]\n",
      "[0.69509335]\n",
      "[0.69477913]\n",
      "[0.69459369]\n",
      "[0.69445083]\n",
      "[0.6943232]\n",
      "[0.6942021]\n",
      "[0.69408473]\n",
      "[0.69397018]\n",
      "[0.69385811]\n",
      "[0.6937484]\n",
      "[0.69364097]\n",
      "[0.69353576]\n",
      "[0.69343272]\n",
      "[0.6933318]\n",
      "[0.69323296]\n",
      "[0.69313616]\n",
      "[0.69304135]\n",
      "[0.6929485]\n",
      "[0.69285755]\n",
      "[0.69276848]\n",
      "[0.69268124]\n",
      "[0.69259579]\n",
      "[0.69251209]\n",
      "[0.69243012]\n",
      "[0.69234982]\n",
      "[0.69227117]\n",
      "[0.69219414]\n",
      "[0.69211868]\n",
      "[0.69204477]\n",
      "[0.69197237]\n",
      "[0.69190145]\n",
      "[0.69183199]\n",
      "[0.69176394]\n",
      "[0.69169728]\n",
      "[0.69163198]\n",
      "[0.69156802]\n",
      "[0.69150535]\n",
      "[0.69144397]\n",
      "[0.69138383]\n",
      "[0.69132492]\n",
      "[0.69126721]\n",
      "[0.69121066]\n",
      "[0.69115527]\n",
      "[0.691101]\n",
      "[0.69104783]\n",
      "[0.69099574]\n",
      "[0.6909447]\n",
      "[0.6908947]\n",
      "[0.6908457]\n",
      "[0.6907977]\n",
      "[0.69075066]\n",
      "[0.69070458]\n",
      "[0.69065942]\n",
      "[0.69061517]\n",
      "[0.69057181]\n",
      "[0.69052932]\n",
      "[0.69048769]\n",
      "[0.69044689]\n",
      "[0.69040691]\n",
      "[0.69036773]\n",
      "[0.69032934]\n",
      "[0.69029171]\n",
      "[0.69025483]\n",
      "[0.69021869]\n",
      "[0.69018327]\n",
      "[0.69014856]\n",
      "[0.69011454]\n",
      "[0.69008119]\n",
      "[0.69004851]\n",
      "[0.69001647]\n",
      "[0.68998507]\n",
      "[0.68995429]\n",
      "[0.68992412]\n",
      "[0.68989455]\n",
      "[0.68986556]\n",
      "[0.68983714]\n",
      "[0.68980928]\n",
      "[0.68978197]\n",
      "[0.68975519]\n",
      "[0.68972894]\n",
      "[0.68970321]\n",
      "[0.68967797]\n",
      "[0.68965324]\n",
      "[0.68962898]\n",
      "[0.6896052]\n",
      "[0.68958188]\n",
      "[0.68955901]\n",
      "[0.68953659]\n",
      "[0.68951461]\n",
      "[0.68949305]\n",
      "[0.68947191]\n",
      "[0.68945117]\n"
     ]
    }
   ],
   "source": [
    "nx, m = 10, 1000 \n",
    "X = np.random.rand(nx*m).reshape((nx, m))\n",
    "y = np.random.randint(low=0, high=2, size=m).reshape((1, m))\n",
    "\n",
    "iterations = 100\n",
    "alpha = 0.5\n",
    "w = np.random.rand(nx).reshape((nx, 1))\n",
    "b = np.random.rand(1).reshape(1,1)\n",
    "for i in range(iterations):\n",
    "    z = np.dot(w.T, X) + b #z.shape()= (1,m) \n",
    "    # np.dot(w.T, X) + b has broadcasting for b: (1,1) --> (1,m)\n",
    "    a = 1/(1+np.exp((-1)*z)) #a.shape() = (1,m)\n",
    "    J = -1/m*np.sum(y*np.log(a)+(1-y)*np.log(1-a), axis=1) #L.shape(1,m)\n",
    "    print(J)\n",
    "    dz = a - y #dz.shape(1,m)\n",
    "    dw = 1/m * np.dot(X, dz.T) #dw.shape(nx, 1)\n",
    "    db = 1/m * np.sum(dz)\n",
    "    w = w - alpha * dw\n",
    "    b = b - alpha * db\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a neural network with arbitrary activation function and hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder**: The general methodology to build a Neural Network is to:\n",
    "    1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "    2. Initialize the model's parameters\n",
    "    3. Loop:\n",
    "        - Implement forward propagation\n",
    "        - Compute loss\n",
    "        - Implement backward propagation to get the gradients\n",
    "        - Update parameters (gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNetwork(object):\n",
    "    \n",
    "    # activation functions with derivatives\n",
    "    SIGMOID = lambda z, derivative=False: 1/(1 + np.exp(-z)) if not derivative else np.exp(-z)/(1+np.exp(-z))**2\n",
    "    TANH = lambda z, derivative=False: np.tanh(z) if not derivative else 1 - np.tanh(z)**2\n",
    "    RELU = lambda z, derivative=False: np.where(z<0, 0, z) if not derivative else np.where(z<0, 0, 1)    \n",
    "    \n",
    "    # it may even works with multiple unit output layer\n",
    "    loss = lambda yhat, y, derivative=False: (-1) * (y * np.log(yhat) + (1-y) * np.log(1-yhat)) if not derivative else\\\n",
    "                                            (-1)*y/yhat + (1-y)/(1-yhat)\n",
    "    cost = lambda yhat, y: 1/y.shape[1] * np.sum(DeepNetwork.loss(yhat, y), axis=1)\n",
    "\n",
    "\n",
    "    def __init__(self, units_per_layer, activation_functions):\n",
    "        assert len(units_per_layer) == len(activation_functions), 'layer mismatch'\n",
    "        self.units_per_layer = list(units_per_layer)\n",
    "        self.activation_functions = list(activation_functions)\n",
    "        self._W = None\n",
    "        self._b = None\n",
    "    \n",
    "    @property\n",
    "    def W(self):\n",
    "        return self._W[1:]\n",
    "    \n",
    "    @property\n",
    "    def b(self):\n",
    "        return self._b[1:]\n",
    "        \n",
    "    def train(self, X, Y, alpha, max_iterations)->np.array:\n",
    "        assert X.shape[1] == Y.shape[1], 'invalid input'\n",
    "        \n",
    "        m = X.shape[1] # number of examples\n",
    "        n = [X.shape[0]] + self.units_per_layer # n[0] is the size of the input layer\n",
    "        G = [None] + self.activation_functions # different functions across layers\n",
    "        L = len(n)-1 # actual number of layers are len(n)-1\n",
    "        \n",
    "        A = [X] + [np.zeros((n[l], m)) for l in range(1, L+1)] # for layer l we have n[l] activations per example\n",
    "        W = [None] + [np.random.randn(n[l], n[l-1])*0.01 for l in range(1, L+1)] # for each layer w.T= W[l]: (n[l], n[l-1])\n",
    "        b = [None] + [np.zeros((n[l], 1)) for l in range(1, L+1)] # each unit has only one b but n[l-1] ws\n",
    "\n",
    "        Z = [None] + [np.zeros((n[l], m)) for l in range(1, L+1)]\n",
    "        dZ = [None] + [np.zeros((n[l], m)) for l in range(1, L+1)]\n",
    "        dW = [None] + [np.zeros((n[l], n[l-1])) for l in range(1, L+1)]\n",
    "        db = [None] + [np.zeros((n[l], 1)) for l in range(1, L+1)]\n",
    "        \n",
    "        cost = float(\"inf\")\n",
    "        for i in range(max_iterations):\n",
    "\n",
    "            # forward propagation from layer 1 to layer L\n",
    "            for l in range(1, L+1):\n",
    "                Z[l] = np.dot(W[l], A[l-1]) + b[l]\n",
    "                A[l] = G[l](Z[l]) # applying activation function for layer l on Z[l]\n",
    "            \n",
    "\n",
    "            # backward propagation from layer L to layer 1    \n",
    "            for l in range(L, 0, -1): # from L <= l <= 1         \n",
    "                if l == L:              \n",
    "                    dZ[L] = DeepNetwork.loss(A[L], Y, derivative=True) * G[L](Z[L], derivative=True)            \n",
    "                else:\n",
    "                    dZ[l] = np.dot(W[l+1].T, dZ[l+1]) * G[l](Z[l], derivative=True)\n",
    "                dW[l] = 1/m * np.dot(dZ[l], A[l-1].T)\n",
    "                db[l] = 1/m * np.sum(dZ[l], axis=1, keepdims=True)\n",
    "\n",
    "                W[l] = W[l] - alpha * dW[l]\n",
    "                b[l] = b[l] - alpha * db[l]\n",
    "            \n",
    "            new_cost = DeepNetwork.cost(A[L], Y)                        \n",
    "            if 0 < cost - new_cost < 0.000001:\n",
    "                cost = new_cost\n",
    "                break\n",
    "            cost = new_cost\n",
    "        \n",
    "        print(i, cost)\n",
    "        self._W = W\n",
    "        self._b = b\n",
    "\n",
    "    def classify(self, X):        \n",
    "        assert X.shape[0] == self._W[1].shape[1], 'invalid input'\n",
    "        from copy import copy\n",
    "        A = X\n",
    "        G = [None] + self.activation_functions        \n",
    "        for l in range(1, len(self._W)):\n",
    "            w = self._W[l]\n",
    "            b = self._b[l]\n",
    "            Z = np.dot(w, A) + b\n",
    "            A = G[l](Z)\n",
    "        return A\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_sphere(X, radius, center):\n",
    "    return np.where(np.sum((X - center)**2, axis=0, keepdims=True) < radius, True, False)\n",
    "\n",
    "def in_diagonal_sphere(X, radius, center):\n",
    "    return in_sphere(X, radius/2, center-radius/2) + in_sphere(X, radius/2, center+radius/2)\n",
    "\n",
    "def in_sphere_sequence(X, radius, center, seq=10):\n",
    "    out = np.zeros((1, X.shape[1]), dtype=bool)\n",
    "    r = radius / seq\n",
    "    for i in range(seq):\n",
    "        out += in_sphere(X, r, center-i*r) + in_sphere(X, r, center+i*r)\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(low=-2, high=2, size=(3,10000))\n",
    "X_test = np.random.uniform(low=-2, high=2, size=(3,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81387549 -0.3141217  -1.54608771 -0.62508154  0.5838078  -0.80966537]\n",
      " [ 1.42665321 -1.99369778  0.50020848  0.78288884  0.65039398  1.56100254]\n",
      " [ 0.36227043  0.0356978   0.20294804 -1.62306782 -0.00607913  0.69989529]] \n",
      " [[False False False False  True False]]\n",
      "7440 [0.03336406]\n",
      "accuracy:  0.985\n",
      "CPU times: user 6min 29s, sys: 20.7 s, total: 6min 50s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Y = in_sphere(X, 1.0, 0.0)\n",
    "print(X[:,:6], '\\n', Y[:, :6])\n",
    "Y_test = in_sphere(X_test, 1.0, 0.0)\n",
    "\n",
    "DN = DeepNetwork\n",
    "dn = DN([4,1], [DN.RELU, DN.SIGMOID])\n",
    "dn.train(X, Y, alpha=0.5, max_iterations=10000)\n",
    "\n",
    "\n",
    "Y_hat = dn.classify(X_test)\n",
    "Y_hat = np.where(Y_hat>=0.5, True, False)\n",
    "\n",
    "print('accuracy: ', np.sum(Y_hat == Y_test)/Y_hat.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81387549 -0.3141217  -1.54608771 -0.62508154  0.5838078  -0.80966537]\n",
      " [ 1.42665321 -1.99369778  0.50020848  0.78288884  0.65039398  1.56100254]\n",
      " [ 0.36227043  0.0356978   0.20294804 -1.62306782 -0.00607913  0.69989529]] \n",
      " [[False False False False  True False]]\n",
      "5356 [0.05263828]\n",
      "accuracy:  0.977\n",
      "CPU times: user 5min 49s, sys: 19.2 s, total: 6min 8s\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Y = in_diagonal_sphere(X, 1.0, 0.0)\n",
    "print(X[:,:6], '\\n', Y[:, :6])\n",
    "Y_test = in_diagonal_sphere(X_test, 1.0, 0.0)\n",
    "\n",
    "DN = DeepNetwork\n",
    "dn = DN([5,1], [DN.RELU,DN.SIGMOID])\n",
    "dn.train(X, Y, alpha=0.5, max_iterations=10000)\n",
    "\n",
    "\n",
    "Y_hat = dn.classify(X_test)\n",
    "Y_hat = np.where(Y_hat>=0.5, True, False)\n",
    "\n",
    "print('accuracy: ', np.sum(Y_hat == Y_test)/Y_hat.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81387549 -0.3141217  -1.54608771 -0.62508154  0.5838078  -0.80966537]\n",
      " [ 1.42665321 -1.99369778  0.50020848  0.78288884  0.65039398  1.56100254]\n",
      " [ 0.36227043  0.0356978   0.20294804 -1.62306782 -0.00607913  0.69989529]] \n",
      " [[False False False False False False]]\n",
      "127 [0.0964907]\n",
      "accuracy:  0.984\n",
      "CPU times: user 13.5 s, sys: 787 ms, total: 14.2 s\n",
      "Wall time: 499 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Y = in_sphere_sequence(X, 1.0, 0.0)\n",
    "print(X[:,:6], '\\n', Y[:, :6])\n",
    "Y_test = in_sphere_sequence(X_test, 1.0, 0.0)\n",
    "\n",
    "DN = DeepNetwork\n",
    "dn = DN([5,1], [DN.SIGMOID,DN.SIGMOID])\n",
    "dn.train(X, Y, alpha=0.5, max_iterations=10000)\n",
    "\n",
    "\n",
    "Y_hat = dn.classify(X_test)\n",
    "Y_hat = np.where(Y_hat>=0.5, True, False)\n",
    "\n",
    "print('accuracy: ', np.sum(Y_hat == Y_test)/Y_hat.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
