{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,) (1, 10) (10, 1)\n",
      "False False\n",
      "(1, 10) (10, 10) (10, 10)\n",
      "(1, 1) (10, 10)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5,6,7,8,9,10]) #rank 1 array which is neither a row vector (1,10) nor a column vector (10, 1)\n",
    "b = np.array([1,2,3,4,5,6,7,8,9,10]).reshape((1,10))\n",
    "c = np.array([1,2,3,4,5,6,7,8,9,10]).reshape((10, 1))\n",
    "print(a.shape, b.shape, c.shape)\n",
    "print(np.array_equal(a, b), np.array_equal(a, c))\n",
    "print((a*b).shape, (a*c).shape, (b*c).shape)\n",
    "print(np.dot(b,c).shape, np.dot(c,b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249966.59497044512\n",
      "Vectorized version:1.7580986022949219ms\n",
      "249966.59497044657\n",
      "For loop:556.1151504516602ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a,b)\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print(\"Vectorized version:\" + str(1000*(toc-tic)) +\"ms\" )\n",
    "\n",
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print(\"For loop:\" + str(1000*(toc-tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(10, 0)\n",
      "(0, 10)\n"
     ]
    }
   ],
   "source": [
    "print(np.zeros(10).shape)\n",
    "print(np.zeros((10, 0)).shape)\n",
    "print(np.zeros((0,10)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized version of Logistic Regression\n",
    "\n",
    "There are three for loops:\n",
    "* The outmost for loop is for the **i iterations**\n",
    "    * second for loop is for the **m training examples**\n",
    "        * third for loop is over the **nx weights**\n",
    "\n",
    "We can vectorize second and third for loops this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.41986636]\n",
      "[1.10393502]\n",
      "[0.88655215]\n",
      "[0.76997461]\n",
      "[0.72108426]\n",
      "[0.7036203]\n",
      "[0.69777363]\n",
      "[0.69580776]\n",
      "[0.69509335]\n",
      "[0.69477913]\n",
      "[0.69459369]\n",
      "[0.69445083]\n",
      "[0.6943232]\n",
      "[0.6942021]\n",
      "[0.69408473]\n",
      "[0.69397018]\n",
      "[0.69385811]\n",
      "[0.6937484]\n",
      "[0.69364097]\n",
      "[0.69353576]\n",
      "[0.69343272]\n",
      "[0.6933318]\n",
      "[0.69323296]\n",
      "[0.69313616]\n",
      "[0.69304135]\n",
      "[0.6929485]\n",
      "[0.69285755]\n",
      "[0.69276848]\n",
      "[0.69268124]\n",
      "[0.69259579]\n",
      "[0.69251209]\n",
      "[0.69243012]\n",
      "[0.69234982]\n",
      "[0.69227117]\n",
      "[0.69219414]\n",
      "[0.69211868]\n",
      "[0.69204477]\n",
      "[0.69197237]\n",
      "[0.69190145]\n",
      "[0.69183199]\n",
      "[0.69176394]\n",
      "[0.69169728]\n",
      "[0.69163198]\n",
      "[0.69156802]\n",
      "[0.69150535]\n",
      "[0.69144397]\n",
      "[0.69138383]\n",
      "[0.69132492]\n",
      "[0.69126721]\n",
      "[0.69121066]\n",
      "[0.69115527]\n",
      "[0.691101]\n",
      "[0.69104783]\n",
      "[0.69099574]\n",
      "[0.6909447]\n",
      "[0.6908947]\n",
      "[0.6908457]\n",
      "[0.6907977]\n",
      "[0.69075066]\n",
      "[0.69070458]\n",
      "[0.69065942]\n",
      "[0.69061517]\n",
      "[0.69057181]\n",
      "[0.69052932]\n",
      "[0.69048769]\n",
      "[0.69044689]\n",
      "[0.69040691]\n",
      "[0.69036773]\n",
      "[0.69032934]\n",
      "[0.69029171]\n",
      "[0.69025483]\n",
      "[0.69021869]\n",
      "[0.69018327]\n",
      "[0.69014856]\n",
      "[0.69011454]\n",
      "[0.69008119]\n",
      "[0.69004851]\n",
      "[0.69001647]\n",
      "[0.68998507]\n",
      "[0.68995429]\n",
      "[0.68992412]\n",
      "[0.68989455]\n",
      "[0.68986556]\n",
      "[0.68983714]\n",
      "[0.68980928]\n",
      "[0.68978197]\n",
      "[0.68975519]\n",
      "[0.68972894]\n",
      "[0.68970321]\n",
      "[0.68967797]\n",
      "[0.68965324]\n",
      "[0.68962898]\n",
      "[0.6896052]\n",
      "[0.68958188]\n",
      "[0.68955901]\n",
      "[0.68953659]\n",
      "[0.68951461]\n",
      "[0.68949305]\n",
      "[0.68947191]\n",
      "[0.68945117]\n"
     ]
    }
   ],
   "source": [
    "nx, m = 10, 1000 \n",
    "X = np.random.rand(nx*m).reshape((nx, m))\n",
    "y = np.random.randint(low=0, high=2, size=m).reshape((1, m))\n",
    "\n",
    "iterations = 100\n",
    "alpha = 0.5\n",
    "w = np.random.rand(nx).reshape((nx, 1))\n",
    "b = np.random.rand(1).reshape(1,1)\n",
    "for i in range(iterations):\n",
    "    z = np.dot(w.T, X) + b #z.shape()= (1,m) \n",
    "    # np.dot(w.T, X) + b has broadcasting for b: (1,1) --> (1,m)\n",
    "    a = 1/(1+np.exp((-1)*z)) #a.shape() = (1,m)\n",
    "    J = -1/m*np.sum(y*np.log(a)+(1-y)*np.log(1-a), axis=1) #L.shape(1,m)\n",
    "    print(J)\n",
    "    dz = a - y #dz.shape(1,m)\n",
    "    dw = 1/m * np.dot(X, dz.T) #dw.shape(nx, 1)\n",
    "    db = 1/m * np.sum(dz)\n",
    "    w = w - alpha * dw\n",
    "    b = b - alpha * db\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a neural network with arbitrary activation function and hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder**: The general methodology to build a Neural Network is to:\n",
    "    1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "    2. Initialize the model's parameters\n",
    "    3. Loop:\n",
    "        - Implement forward propagation\n",
    "        - Compute loss\n",
    "        - Implement backward propagation to get the gradients\n",
    "        - Update parameters (gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69315259]\n",
      "[0.6923522]\n",
      "[0.69159134]\n",
      "[0.69086806]\n",
      "[0.6901805]\n",
      "[0.68952688]\n",
      "[0.68890554]\n",
      "[0.68831486]\n",
      "[0.68775333]\n",
      "[0.68721951]\n"
     ]
    }
   ],
   "source": [
    "# activation functions with derivatives\n",
    "sigmoid = lambda z, derivative=False: 1/(1 + np.exp(-z)) if not derivative else np.exp(-z)/(1+np.exp(-z))**2\n",
    "tanh = lambda z, derivative=False: np.tanh(z) if not derivative else 1 - np.tanh(z)**2\n",
    "\n",
    "# now it works with multiple unit output layer\n",
    "loss = lambda yhat, y, derivative=False: (-1) * (y * np.log(yhat) + (1-y) * np.log(1-yhat)) if not derivative else\\\n",
    "                                        -y/yhat + (1-y)/(1-yhat)\n",
    "cost = lambda yhat, y: 1/m * np.sum(loss(yhat, y), axis=1)\n",
    "\n",
    "m = 100 # number of examples\n",
    "n = [4, 3, 1] # n[0] is the size of the input layer\n",
    "L = len(n)-1 # actual number of layers are len(n)-1\n",
    "G = [None, tanh, sigmoid] # using separate functions for each layer\n",
    "\n",
    "Y = np.random.randint(low=0, high=2, size=(1,m))\n",
    "X = np.random.rand(n[0], m) # layer zero activation\n",
    "\n",
    "A = [X] + [np.zeros((n[l], m)) for l in range(1, L+1)] # for layer l we have n[l] activations per example\n",
    "W = [None] + [np.random.randn(n[l], n[l-1])*0.01 for l in range(1, L+1)] # for each layer w.T= W[l]: (n[l], n[l-1])\n",
    "b = [None] + [np.zeros((n[l], 1)) for l in range(1, L+1)] # each unit has only one b but n[l-1] ws\n",
    "\n",
    "Z = [None] + [np.zeros((n[l], m)) for l in range(1, L+1)]\n",
    "dZ = [None] + [np.zeros((n[l], m)) for l in range(1, L+1)]\n",
    "dW = [None] + [np.zeros((n[l], n[l-1])) for l in range(1, L+1)]\n",
    "db = [None] + [np.zeros((n[l], 1)) for l in range(1, L+1)]\n",
    "\n",
    "alpha = 0.1\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    \n",
    "    # forward propagation from layer 1 to layer L\n",
    "    for l in range(1, L+1):\n",
    "        Z[l] = np.dot(W[l], A[l-1]) + b[l]\n",
    "        A[l] = G[l](Z[l]) # applying activation function for layer l on Z[l]\n",
    "    \n",
    "    print(cost(A[L], Y))\n",
    "        \n",
    "    # backward propagation from layer L to layer 1    \n",
    "    for l in range(L, 0, -1): # from L <= l <= 1         \n",
    "        if l == L:              \n",
    "            dZ[L] = loss(A[L], Y, derivative=True) * G[L](Z[L], derivative=True)            \n",
    "        else:\n",
    "            dZ[l] = np.dot(W[l+1].T, dZ[l+1]) * G[l](Z[l], derivative=True)\n",
    "        dW[l] = 1/m * np.dot(dZ[l], A[l-1].T)\n",
    "        db[l] = 1/m * np.sum(dZ[l], axis=1, keepdims=True)\n",
    "        \n",
    "        W[l] = W[l] - alpha * dW[l]\n",
    "        b[l] = b[l] - alpha * db[l]\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
